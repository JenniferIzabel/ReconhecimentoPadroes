{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de padrões "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos o código com as importações das bibliotecas que serão necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "from re import search\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from mne import set_eeg_reference as car\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificando as pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names =  [\n",
    "            'FP1','FP2','F7','F8','AF1','AF2','FZ','F4','F3','FC6','FC5','FC2','FC1',\n",
    "          ≈                                    'T8','T7','CZ','C3','C4','CP5','CP6','CP1','CP2','P3','P4','PZ','P8','P7','PO2','PO1',\n",
    "            'O2','O1','X','AF7','AF8','F5','F6','FT7','FT8','FPZ','FC4','FC3','C6','C5','F2','F1',\n",
    "            'TP8','TP7','AFZ','CP3','CP4','P5','P6','C1','C2','PO7','PO8','FCZ','POZ','OZ','P2','P1','CPZ','nd','Y'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "    'full': 'dataset/full'\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Modificação da função fornecida pelo professor afim de recuperar e separar a base pelos testes S1_obj, S2_nomatch, S2_match. \n",
    "cada teste tem 64 eletrodos com 256 leituras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datas(files, tests, path_files, types):\n",
    "    \n",
    "    trials = list()\n",
    "    for f in files:\n",
    "        arquivo = open('{}/{}/{}'.format(folders[path_files], types, f))\n",
    "        text = arquivo.readlines()\n",
    "        # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "        chs = list()\n",
    "\n",
    "        # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "        values = list()\n",
    "        for line in text:\n",
    "            # ex: \"# FP1 chan 0\"\n",
    "            t = search('\\w{1,3} chan \\d{1,2}', line)\n",
    "\n",
    "            # ex: \"0 FP1 0 -8.921\"\n",
    "            p = search('^\\d{1,2}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "            if p:\n",
    "                values.append(float(p.group('value')))\n",
    "            # mudou para outro eletrodo\n",
    "            elif t and values:\n",
    "                chs.append(values)\n",
    "                values = list()\n",
    "        chs.append(values)\n",
    "        arquivo.seek(32*3)\n",
    "        line =  arquivo.readline()\n",
    "        \n",
    "        if \"S1 obj\" in line:\n",
    "            if len(chs) != 1:   \n",
    "                tests[\"S1_obj\"].append(chs)\n",
    "\n",
    "        elif \"S2 nomatch\" in line:\n",
    "            if len(chs) != 1:\n",
    "                tests[\"S2_nomatch\"].append(chs)\n",
    "\n",
    "        elif \"S2 match\" in line:\n",
    "            if len(chs) != 1:\n",
    "                tests[\"S2_match\"].append(chs)\n",
    "        \n",
    "        arquivo.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nesta função são identificados e separados os casos de alcoolicos e controle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bases2(path_files):\n",
    "    diretory = gop('ls {}'.format(folders[path_files])).split('\\n')\n",
    "\n",
    "    subA = {\"S1_obj\":[], \"S2_nomatch\":[], \"S2_match\":[]}\n",
    "    subC = {\"S1_obj\":[], \"S2_nomatch\":[], \"S2_match\":[]}\n",
    "\n",
    "    for types in diretory:\n",
    "        files = gop('ls {}/{}'.format(folders[path_files], types)).split('\\n')\n",
    "        if 'Co2A' in types.title():\n",
    "            get_all_datas(files, subA, path_files, types)\n",
    "        else:\n",
    "            get_all_datas(files, subC, path_files, types)\n",
    "\n",
    "    return [subA, subC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função auxiliar responsável por realizar a média e entre os 64 eletrodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_pre_proc(data):\n",
    "    new_raw = []\n",
    "\n",
    "    ch_types = ['eeg'] * 64\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=256, ch_types=ch_types)\n",
    "\n",
    "    for i in data:\n",
    "        raw = mne.io.RawArray(i, info, verbose= False)\n",
    "        #raw.drop_channels(['X', 'nd', 'Y'])\n",
    "        inst, data = car(raw, ref_channels='average', verbose= False)\n",
    "        new_raw.append(data)\n",
    "\n",
    "    return new_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função resposável por realizar as médias entre os eletrodos, bem como designar as respectivas classes juntamente com o embaralhamento das entradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pros(data):\n",
    "    # recebe os valores referentes aos alcoolatras\n",
    "    alco = data[0]\n",
    "\n",
    "    # valores referentes ao controle\n",
    "    contro = data[1]\n",
    "\n",
    "    alco = aux_pre_proc(alco)\n",
    "    contro = aux_pre_proc(contro)\n",
    "\n",
    "    # identifica a quantidade de classes (alcoolatras e controles) e mistura todas elas para que seja possível treinar a rede neural\n",
    "    classes = list()\n",
    "    for i in range(0, len(alco)):\n",
    "        classes.append(1)\n",
    "    for i in range(0, len(contro)):\n",
    "        classes.append(0)\n",
    "\n",
    "    total = alco + contro\n",
    "    \n",
    "    # combina as classes com as suas respectivas entradas para não perder a posição respectivas de ambos\n",
    "    combined = list(zip(total, classes))\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    total[:], classes[:] = zip(*combined)\n",
    "\n",
    "    return [np.asarray(total), np.asarray(classes)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função responsável pelo treino da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data, name):\n",
    "    print(data[0].shape)\n",
    "    classifier = Sequential()\n",
    "\n",
    "    classifier.add(Dense(units = 50, activation = 'relu', input_dim = 256))\n",
    "    classifier.add(Dense(units = 30, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    classifier.fit(data[0], data[1], nb_epoch = 15)\n",
    "    classifier.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função responsável por realizar os testes nos respectivos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test, name_model):\n",
    "\n",
    "    model = load_model(name_model)\n",
    "    prev = model.predict(test[0])\n",
    "\n",
    "    prev = (prev > 0.50)\n",
    "    print(\"RESULTADO PARA O TESTE \"+name_model+\": \"+str(accuracy_score(prev, test[1])))\n",
    "    matrix = confusion_matrix(prev, test[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função que envia os respectivos dados á função de treino\n",
    "def train(train, name_test):\n",
    "    name = name_test+\".h5\"\n",
    "\n",
    "    train_processed = pre_pros([train[0][name_test], train[1][name_test]])\n",
    "    training(train_processed, name)\n",
    "\n",
    "    \n",
    "# função que envia os respectivos dados á função de teste\n",
    "def test(test, name_test):\n",
    "    name = name_test+\".h5\"\n",
    "\n",
    "    test_processed = pre_pros([test[0][name_test], test[1][name_test]])\n",
    "    test_model(test_processed, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tests =  [\"S1_obj\", \"S2_nomatch\", \"S2_match\"]\n",
    "\n",
    "    data_train = load_bases2('large_train')\n",
    "    data_test = load_bases2('large_test')\n",
    "\n",
    "\n",
    "    for i in tests:\n",
    "        train(data_train, i)\n",
    "    \n",
    "    for i in tests:\n",
    "        test(data_test, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A fim de comparação a mesma configuração foi utilizada com a base \"full\"\n",
    "\n",
    "os ultimos resultado são:\n",
    "     S1 obj: 88%\n",
    "     S2 nomatch: 91%\n",
    "     S2 match: 87%\n",
    "     \n",
    " Quando o treinamento foi realizado com a base \"large_train\" os resultados cairam drasticamente:\n",
    " os ultimos resultados:\n",
    "     S1 obj: 55%\n",
    "     S2 nomatch: 62%\n",
    "     S2 match: 60%\n",
    " demonstrando que quando há poucos dados para treino, a rede neural acaba sendo prejudicada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
